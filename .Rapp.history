img <- child$img
thumbnail <- xmlGetAttr(img, "src")
title <- xmlGetAttr(img, "title")
alt <- xmlGetAttr(img, "alt")
alt
l <- readLines("/Users/brian/Dropbox/viewR/scrapers/imdb/data/genreLIST.txt")
imdb_ids <- function(genre) {
if(!require('XML')){
install.packages('XML')
library('XML')
}
if(!require('RCurl')){
install.packages('RCurl')
library('RCurl')
}
if(!require('stringr')){
install.packages('stringr')
library('stringr')
}
if(!require('plyr')){
install.packages('plyr')
library('plyr')
}
if(!require('plyr')){
install.packages('plyr')
library('plyr')
}
if(!require('rjson')){
install.packages('rjson')
library('rjson')
}
extract page count
genre <- as.character(genre)
root <- "http://www.imdb.com/search/title?genres="
part1 <- "&sort=release_date_us,desc&start="
part2 <- "&title_type=feature"
url1 <- paste0(root, genre, part1, 1, part2)
page <- getURL(url1)
tree <- htmlTreeParse(page, useInternalNodes=T)
raw_n_pages <- xmlValue(getNodeSet(tree, '//*[@id="left"]')[[1]])
raw_n_pages <- gsub("\n1-[0-9]{1,2} of ([0-9,]+)\ntitles\\.\n", "\\1", raw_npages)
n_pages <- as.numeric(gsub(",", "", raw_n_pages))
generate urls
if(n_pages > 50) {
urls <- paste0(root, genre, seq(51, n_pages, 50), part2)
} else {
urls <- url1
}
scraping function
scrape_imdb_ids <- function(url){
page <- getURL(url)
tree <- htmlTreeParse(page, useInternalNodes=T)
get titles, ids, dates, img_url
imageNodes <- getNodeSet(tree, '//*[@class="image"]/a')
imdb_id <- laply(imageNodes, function(title) {xmlGetAttr(title, "href")})
child <- xmlChildren(imageNodes[[1]])
img <- child$img
thumbnail <- xmlGetAttr(img, "src")
title <- xmlGetAttr(img, "title")
alt <- xmlGetAttr(img, "alt")
return(data.frame(imdb_id,
title,
alt,
thumhbnail,
stringsAsFactors=F))
}
ids <- as.character(urls)
output <- llply(ids, function(id) {
out <- try(scrape_imdb_ids(id), TRUE)
if (class(out)=='try-error') {
print()
warning(paste("error scraping", id))
out <- NULL
} else {
return(out)
}
}, .progress="text")
remove null elements, combine in one data.frame
output <- output[!sapply(output, is.null)]
print("reducing output...")
output <- ldply(output, function(x) {
data.frame(x, stringsAsFactors=FALSE)
}, .progress="text")
return(output)
}
l[5]
scrape_imdb_ids(l[5])
l <- readLines("/Users/brian/Dropbox/viewR/scrapers/imdb/data/genreLIST.txt")
imdb_ids <- function(genre) {
if(!require('XML')){
install.packages('XML')
library('XML')
}
if(!require('RCurl')){
install.packages('RCurl')
library('RCurl')
}
if(!require('stringr')){
install.packages('stringr')
library('stringr')
}
if(!require('plyr')){
install.packages('plyr')
library('plyr')
}
if(!require('plyr')){
install.packages('plyr')
library('plyr')
}
if(!require('rjson')){
install.packages('rjson')
library('rjson')
}
extract page count
genre <- as.character(genre)
root <- "http://www.imdb.com/search/title?genres="
part1 <- "&sort=release_date_us,desc&start="
part2 <- "&title_type=feature"
url1 <- paste0(root, genre, part1, 1, part2)
page <- getURL(url1)
tree <- htmlTreeParse(page, useInternalNodes=T)
raw_n_pages <- xmlValue(getNodeSet(tree, '//*[@id="left"]')[[1]])
raw_n_pages <- gsub("\n1-[0-9]{1,2} of ([0-9,]+)\ntitles\\.\n", "\\1", raw_npages)
n_pages <- as.numeric(gsub(",", "", raw_n_pages))
generate urls
if(n_pages > 50) {
urls <- paste0(root, genre, seq(51, n_pages, 50), part2)
} else {
urls <- url1
}
scraping function
scrape_imdb_ids <- function(url){
page <- getURL(url)
tree <- htmlTreeParse(page, useInternalNodes=T)
get titles, ids, dates, img_url
imageNodes <- getNodeSet(tree, '//*[@class="image"]/a')
imdb_id <- laply(imageNodes, function(title) {xmlGetAttr(title, "href")})
child <- xmlChildren(imageNodes[[1]])
img <- child$img
thumbnail <- xmlGetAttr(img, "src")
title <- xmlGetAttr(img, "title")
alt <- xmlGetAttr(img, "alt")
return(data.frame(imdb_id,
title,
alt,
thumhbnail,
stringsAsFactors=F))
}
ids <- as.character(urls)
output <- llply(ids, function(id) {
out <- try(scrape_imdb_ids(id), TRUE)
if (class(out)=='try-error') {
print()
warning(paste("error scraping", id))
out <- NULL
} else {
return(out)
}
}, .progress="text")
remove null elements, combine in one data.frame
output <- output[!sapply(output, is.null)]
print("reducing output...")
output <- ldply(output, function(x) {
data.frame(x, stringsAsFactors=FALSE)
}, .progress="text")
return(output)
}
l[5]
imdb_ids(l[5])
l <- readLines("/Users/brian/Dropbox/viewR/scrapers/imdb/data/genreLIST.txt")
imdb_ids <- function(genre) {
if(!require('XML')){
install.packages('XML')
library('XML')
}
if(!require('RCurl')){
install.packages('RCurl')
library('RCurl')
}
if(!require('stringr')){
install.packages('stringr')
library('stringr')
}
if(!require('plyr')){
install.packages('plyr')
library('plyr')
}
if(!require('plyr')){
install.packages('plyr')
library('plyr')
}
if(!require('rjson')){
install.packages('rjson')
library('rjson')
}
extract page count
genre <- as.character(genre)
root <- "http://www.imdb.com/search/title?genres="
part1 <- "&sort=release_date_us,desc&start="
part2 <- "&title_type=feature"
url1 <- paste0(root, genre, part1, 1, part2)
page <- getURL(url1)
tree <- htmlTreeParse(page, useInternalNodes=T)
raw_n_pages <- xmlValue(getNodeSet(tree, '//*[@id="left"]')[[1]])
raw_n_pages <- gsub("\n1-[0-9]{1,2} of ([0-9,]+)\ntitles\\.\n", "\\1", raw_npages)
n_pages <- as.numeric(gsub(",", "", raw_n_pages))
generate urls
if(n_pages > 50) {
urls <- paste0(root, genre, seq(51, n_pages, 50), part2)
} else {
urls <- url1
}
scraping function
scrape_imdb_ids <- function(url){
page <- getURL(url)
tree <- htmlTreeParse(page, useInternalNodes=T)
get titles, ids, dates, img_url
imageNodes <- getNodeSet(tree, '//*[@class="image"]/a')
imdb_id <- laply(imageNodes, function(title) {xmlGetAttr(title, "href")})
child <- xmlChildren(imageNodes[[1]])
img <- child$img
thumbnail <- xmlGetAttr(img, "src")
title <- xmlGetAttr(img, "title")
alt <- xmlGetAttr(img, "alt")
return(data.frame(imdb_id,
title,
alt,
thumhbnail,
stringsAsFactors=F))
}
ids <- as.character(urls)
output <- llply(ids, function(id) {
out <- try(scrape_imdb_ids(id), TRUE)
if (class(out)=='try-error') {
warning(print(paste("error scraping", id)))
out <- NULL
} else {
return(out)
}
}, .progress="text")
remove null elements, combine in one data.frame
output <- output[!sapply(output, is.null)]
print("reducing output...")
output <- ldply(output, function(x) {
data.frame(x, stringsAsFactors=FALSE)
}, .progress="text")
return(output)
}
l[5]
imdb_ids(l[5])
